var documenterSearchIndex = {"docs":
[{"location":"reference/#Reference-1","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/#","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/#Contents-1","page":"Reference","title":"Contents","text":"","category":"section"},{"location":"reference/#","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/#","page":"Reference","title":"Reference","text":"Pages = [\"reference.md\"]","category":"page"},{"location":"reference/#","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/#Index-1","page":"Reference","title":"Index","text":"","category":"section"},{"location":"reference/#","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/#","page":"Reference","title":"Reference","text":"Pages = [\"reference.md\"]","category":"page"},{"location":"reference/#","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/#","page":"Reference","title":"Reference","text":"Modules = [SolverBenchmark]","category":"page"},{"location":"reference/#Base.join-Tuple{Dict{Symbol, DataFrames.DataFrame}, Vector{Symbol}}","page":"Reference","title":"Base.join","text":"df = join(stats, cols; kwargs...)\n\nJoin a dictionary of DataFrames given by stats. Column :id is required in all DataFrames. The resulting DataFrame will have column id and all columns cols for each solver.\n\nInputs:\n\nstats::Dict{Symbol,DataFrame}: Dictionary of DataFrames per solver. Each key is a different solver;\ncols::Array{Symbol}: Which columns of the DataFrames.\n\nKeyword arguments:\n\ninvariant_cols::Array{Symbol,1}: Invariant columns to be added, i.e., columns that don't change depending on the solver (such as name of problem, number of variables, etc.);\nhdr_override::Dict{Symbol,String}: Override header names.\n\nOutput:\n\ndf::DataFrame: Resulting dataframe.\n\n\n\n\n\n","category":"method"},{"location":"reference/#BenchmarkProfiles.performance_profile-Tuple{Dict{Symbol, DataFrames.DataFrame}, Function, Vararg{Any}}","page":"Reference","title":"BenchmarkProfiles.performance_profile","text":"performance_profile(stats, cost)\n\nProduce a performance profile comparing solvers in stats using the cost function.\n\nInputs:\n\nstats::Dict{Symbol,DataFrame}: pairs of :solver => df;\ncost::Function: cost function applyed to each df. Should return a vector with the cost of solving the problem at each row;\n0 cost is not allowed;\nIf the solver did not solve the problem, return Inf or a negative number.\n\nExamples of cost functions:\n\ncost(df) = df.elapsed_time: Simple elapsed_time cost. Assumes the solver solved the problem.\ncost(df) = (df.status .!= :first_order) * Inf + df.elapsed_time: Takes into consideration the status of the solver.\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.bmark_results_to_dataframes-Tuple{PkgBenchmark.BenchmarkResults}","page":"Reference","title":"SolverBenchmark.bmark_results_to_dataframes","text":"stats = bmark_results_to_dataframes(results)\n\nConvert PkgBenchmark results to a dictionary of DataFrames. The benchmark SUITE should have been constructed in the form\n\nSUITE[solver][case] = ...\n\nwhere solver will be recorded as one of the solvers to be compared in the DataFrame and case is a test case. For example:\n\nSUITE[\"CG\"][\"BCSSTK09\"] = @benchmarkable ...\nSUITE[\"LBFGS\"][\"ROSENBR\"] = @benchmarkable ...\n\nInputs:\n\nresults::BenchmarkResults: the result of PkgBenchmark.benchmarkpkg\n\nOutput:\n\nstats::Dict{Symbol,DataFrame}: a dictionary of DataFrames containing the   benchmark results per solver.\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.bmark_solvers-Tuple{Dict{Symbol}, Vararg{Any}}","page":"Reference","title":"SolverBenchmark.bmark_solvers","text":"bmark_solvers(solvers :: Dict{Symbol,Any}, args...; kwargs...)\n\nRun a set of solvers on a set of problems.\n\nArguments\n\nsolvers: a dictionary of solvers to which each problem should be passed\nother positional arguments accepted by solve_problems, except for a solver name\n\nKeyword arguments\n\nAny keyword argument accepted by solve_problems\n\nReturn value\n\nA Dict{Symbol, AbstractExecutionStats} of statistics.\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.count_unique-Tuple{Any}","page":"Reference","title":"SolverBenchmark.count_unique","text":"vals = count_unique(X)\n\nCount the number of occurrences of each value in X.\n\nArguments\n\nX: an iterable.\n\nReturn value\n\nA Dict{eltype(X),Int} whose keys are the unique elements in X and values are their number of occurrences.\n\nExample: the snippet\n\nstats = load_stats(\"mystats.jld2\")\nfor solver ∈ keys(stats)\n  @info \"$solver statuses\" count_unique(stats[solver].status)\nend\n\ndisplays the number of occurrences of each final status for each solver in stats.\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.gradient_highlighter-Tuple{DataFrames.DataFrame, Symbol}","page":"Reference","title":"SolverBenchmark.gradient_highlighter","text":"hl = gradient_highlighter(df, col; cmap=:coolwarm)\n\nA PrettyTables highlighter the applies a color gradient to the values in columns given by cols.\n\nInput Arguments\n\ndf::DataFrame dataframe to which the highlighter will be applied;\ncol::Symbol a symbol to indicate which column the highlighter will be applied to.\n\nKeyword Arguments\n\ncmap::Symbol color scheme to use, from ColorSchemes.\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.judgement_results_to_dataframes-Tuple{PkgBenchmark.BenchmarkJudgement}","page":"Reference","title":"SolverBenchmark.judgement_results_to_dataframes","text":"stats = judgement_results_to_dataframes(judgement)\n\nConvert BenchmarkJudgement results to a dictionary of DataFrames.\n\nInputs:\n\njudgement::BenchmarkJudgement: the result of, e.g.,\ncommit = benchmarkpkg(mypkg)  # benchmark a commit or pull request\nmain = benchmarkpkg(mypkg, \"main\")  # baseline benchmark\njudgement = judge(commit, main)\n\nOutput:\n\nstats::Dict{Symbol,Dict{Symbol,DataFrame}}: a dictionary of   Dict{Symbol,DataFrame}s containing the target and baseline benchmark results.   The elements of this dictionary are the same as those returned by   bmark_results_to_dataframes(main) and bmark_results_to_dataframes(commit).\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.load_stats-Tuple{AbstractString}","page":"Reference","title":"SolverBenchmark.load_stats","text":"stats = load_stats(filename; kwargs...)\n\nArguments\n\nfilename::AbstractString: the input file name.\n\nKeyword arguments\n\nkey::String=\"stats\": the key under which the data can be read in filename. The key should be the same as the one used when save_stats was called.\n\nReturn value\n\nA Dict{Symbol,DataFrame} containing the statistics stored in file filename. The user should import DataFrames before calling load_stats.\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.passfail_highlighter","page":"Reference","title":"SolverBenchmark.passfail_highlighter","text":"hl = passfail_highlighter(df, c=crayon\"bold red\")\n\nA PrettyTables highlighter that colors failures in bold red by default.\n\nInput Arguments\n\ndf::DataFrame dataframe to which the highlighter will be applied.   df must have the id column.\n\nIf df has the :status property, the highlighter will be applied to rows for which df.status indicates a failure. A failure is any status different from :first_order or :unbounded.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SolverBenchmark.passfail_latex_highlighter","page":"Reference","title":"SolverBenchmark.passfail_latex_highlighter","text":"hl = passfail_latex_highlighter(df)\n\nA PrettyTables LaTeX highlighter that colors failures in bold red by default.\n\nSee the documentation of passfail_highlighter for more information.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SolverBenchmark.pretty_latex_stats-Tuple{IO, DataFrames.DataFrame}","page":"Reference","title":"SolverBenchmark.pretty_latex_stats","text":"pretty_latex_stats(df; kwargs...)\n\nPretty-print a DataFrame as a LaTeX longtable using PrettyTables.\n\nSee the pretty_stats documentation. Specific settings in this method are:\n\nthe backend is set to :latex;\nthe table type is set to :longtable;\nhighlighters, if any, should be LaTeX highlighters.\n\nSee the PrettyTables documentation for more information.\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.pretty_stats-Tuple{IO, DataFrames.DataFrame}","page":"Reference","title":"SolverBenchmark.pretty_stats","text":"pretty_stats(df; kwargs...)\n\nPretty-print a DataFrame using PrettyTables.\n\nArguments\n\nio::IO: an IO stream to which the table will be output (default: stdout);\ndf::DataFrame: the DataFrame to be displayed. If only certain columns of df should be displayed,     they should be extracted explicitly, e.g., by passing df[!, [:col1, :col2, :col3]].\n\nKeyword Arguments\n\ncol_formatters::Dict{Symbol, String}: a Dict of format strings to apply to selected columns of df.     The keys of col_formatters should be symbols, so that specific formatting can be applied to specific columns.     By default, default_formatters is used, based on the column type.     If PrettyTables formatters are passed using the formatters keyword argument, they are applied     before those in col_formatters.\nhdr_override::Dict{Symbol, String}: a Dict of those headers that should be displayed differently than     simply according to the column name (default: empty). Example: Dict(:col1 => \"column 1\").\n\nAll other keyword arguments are passed directly to pretty_table. In particular,\n\nuse tf=tf_markdown to display a Markdown table;\ndo not use this function for LaTeX output; use pretty_latex_stats instead;\nany PrettyTables highlighters can be given, but see the predefined passfail_highlighter and gradient_highlighter.\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.profile_package-Tuple{PkgBenchmark.BenchmarkJudgement}","page":"Reference","title":"SolverBenchmark.profile_package","text":"p = profile_package(judgement)\n\nProduce performance profiles based on PkgBenchmark.BenchmarkJudgement results.\n\nInputs:\n\njudgement::BenchmarkJudgement: the result of, e.g.,\ncommit = benchmarkpkg(mypkg)  # benchmark a commit or pull request\nmain = benchmarkpkg(mypkg, \"main\")  # baseline benchmark\njudgement = judge(commit, main)\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.profile_solvers-Tuple{Dict{Symbol, DataFrames.DataFrame}, Vector{<:Function}, Vector{String}}","page":"Reference","title":"SolverBenchmark.profile_solvers","text":"p = profile_solvers(stats, costs, costnames)\n\nProduce performance profiles comparing solvers based on the data in stats.\n\nInputs:\n\nstats::Dict{Symbol,DataFrame}: a dictionary of DataFrames containing the   benchmark results per solver (e.g., produced by bmark_results_to_dataframes())\ncosts::Vector{Function}: a vector of functions specifying the measures to use in the profiles\ncostnames::Vector{String}: names to be used as titles of the profiles.\n\nKeyword inputs:\n\nwidth::Int: Width of each individual plot (Default: 400)\nheight::Int: Height of each individual plot (Default: 400)\n\nOutput: A Plots.jl plot representing a set of performance profiles comparing the solvers. The set contains performance profiles comparing all the solvers together on the measures given in costs. If there are more than two solvers, additional profiles are produced comparing the solvers two by two on each cost measure.\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.profile_solvers-Tuple{PkgBenchmark.BenchmarkResults}","page":"Reference","title":"SolverBenchmark.profile_solvers","text":"p = profile_solvers(results)\n\nProduce performance profiles based on PkgBenchmark.benchmarkpkg results.\n\nInputs:\n\nresults::BenchmarkResults: the result of PkgBenchmark.benchmarkpkg.\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.quick_summary-Tuple{Dict{Symbol, DataFrames.DataFrame}}","page":"Reference","title":"SolverBenchmark.quick_summary","text":"statuses, avgs = quick_summary(stats; kwargs...)\n\nCall count_unique and compute a few average measures for each solver in stats.\n\nArguments\n\nstats::Dict{Symbol,DataFrame}: benchmark statistics such as returned by bmark_solvers.\n\nKeyword arguments\n\ncols::Vector{Symbol}: symbols indicating DataFrame columns in solver statistics for which we compute averages. Default: [:iter, :neval_obj, :neval_grad, :neval_hess, :neval_hprod, :elapsed_time].\n\nReturn value\n\nstatuses::Dict{Symbol,Dict{Symbol,Int}}: a dictionary of number of occurrences of each final status for each solver in stats. Each value in this dictionary is returned by count_unique\navgs::Dict{Symbol,Dict{Symbol,Float64}}: a dictionary that contains averages of performance measures across all problems for each solver. Each avgs[solver] is a Dict{Symbol,Float64} where the measures are those given in the keyword argument cols and values are averages of those measures across all problems.\n\nExample: the snippet\n\nstatuses, avgs = quick_summary(stats)\nfor solver ∈ keys(stats)\n  @info \"statistics for\" solver statuses[solver] avgs[solver]\nend\n\ndisplays quick summary and averages for each solver.\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.save_stats-Tuple{Dict{Symbol, DataFrames.DataFrame}, AbstractString}","page":"Reference","title":"SolverBenchmark.save_stats","text":"save_stats(stats, filename; kwargs...)\n\nWrite the benchmark statistics stats to a file named filename.\n\nArguments\n\nstats::Dict{Symbol,DataFrame}: benchmark statistics such as returned by bmark_solvers\nfilename::AbstractString: the output file name.\n\nKeyword arguments\n\nforce::Bool=false: whether to overwrite filename if it already exists\nkey::String=\"stats\": the key under which the data can be read from filename later.\n\nReturn value\n\nThis method returns an error if filename exists and force==false. On success, it returns the value of jldopen(filename, \"w\").\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.solve_problems-Tuple{Any, Any}","page":"Reference","title":"SolverBenchmark.solve_problems","text":"solve_problems(solver, problems; kwargs...)\n\nApply a solver to a set of problems.\n\nArguments\n\nsolver: the function name of a solver;\nproblems: the set of problems to pass to the solver, as an iterable of AbstractNLPModel. It is recommended to use a generator expression (necessary for CUTEst problems).\n\nKeyword arguments\n\nsolver_logger::AbstractLogger: logger wrapping the solver call (default: NullLogger);\nreset_problem::Bool: reset the problem's counters before solving (default: true);\nskipif::Function: function to be applied to a problem and return whether to skip it (default: x->false);\ncolstats::Vector{Symbol}: summary statistics for the logger to output during the\n\nbenchmark (default: [:name, :nvar, :ncon, :status, :elapsed_time, :objective, :dual_feas, :primal_feas]);\n\ninfo_hdr_override::Dict{Symbol,String}: header overrides for the summary statistics (default: use default headers);\nprune: do not include skipped problems in the final statistics (default: true);\nany other keyword argument to be passed to the solver.\n\nReturn value\n\na DataFrame where each row is a problem, minus the skipped ones if prune is true.\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.to_gist-Tuple{Any}","page":"Reference","title":"SolverBenchmark.to_gist","text":"posted_gist = to_gist(results)\n\nCreate and post a gist with the benchmark results and performance profiles.\n\nInputs:\n\nresults::BenchmarkResults: the result of PkgBenchmark.benchmarkpkg\n\nOutput:\n\nthe return value of GitHub.jl's create_gist.\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.to_gist-Tuple{PkgBenchmark.BenchmarkResults, Any}","page":"Reference","title":"SolverBenchmark.to_gist","text":"posted_gist = to_gist(results, p)\n\nCreate and post a gist with the benchmark results and performance profiles.\n\nInputs:\n\nresults::BenchmarkResults: the result of PkgBenchmark.benchmarkpkg\np:: the result of profile_solvers.\n\nOutput:\n\nthe return value of GitHub.jl's create_gist.\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.LTXformat","page":"Reference","title":"SolverBenchmark.LTXformat","text":"LTXformat(x)\n\nFormat x according to its type. For types Signed, AbstractFloat, AbstractString and Symbol, use a predefined formatting string passed to @sprintf and then the corresponding safe_latex_<type> function.\n\nFor type Missing, return \"NA\".\n\n\n\n\n\n","category":"function"},{"location":"reference/#SolverBenchmark.MDformat","page":"Reference","title":"SolverBenchmark.MDformat","text":"MDformat(x)\n\nFormat x according to its type. For types Signed, AbstractFloat, AbstractString and Symbol, use a predefined formatting string passed to @sprintf.\n\nFor type Missing, return \"NA\".\n\n\n\n\n\n","category":"function"},{"location":"reference/#SolverBenchmark.format_table-Union{Tuple{F}, Tuple{DataFrames.DataFrame, Function}} where F<:Function","page":"Reference","title":"SolverBenchmark.format_table","text":"format_table(df, formatter, kwargs...)\n\nFormat the data frame into a table using formatter. Used by other table functions.\n\nInputs:\n\ndf::DataFrame: Dataframe of a solver. Each row is a problem.\nformatter::Function: A function that formats its input according to its type. See LTXformat or MDformat for examples.\n\nKeyword arguments:\n\ncols::Array{Symbol}: Which columns of the df. Defaults to using all columns;\nignore_missing_cols::Bool: If true, filters out the columns in cols that don't exist in the data frame. Useful when creating tables for solvers in a loop where one solver has a column the other doesn't. If false, throws BoundsError in that situation.\nfmt_override::Dict{Symbol,Function}: Overrides format for a specific column, such as\nfmt_override=Dict(:name => x->@sprintf(\"%-10s\", x))\nhdr_override::Dict{Symbol,String}: Overrides header names, such as hdr_override=Dict(:name => \"Name\").\n\nOutputs:\n\nheader::Array{String,1}: header vector.\ntable::Array{String,2}: formatted table.\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.latex_table-Tuple{IO, DataFrames.DataFrame}","page":"Reference","title":"SolverBenchmark.latex_table","text":"latex_table(io, df, kwargs...)\n\nCreate a latex longtable of a DataFrame using LaTeXTabulars, and format the output for a publication-ready table.\n\nInputs:\n\nio::IO: where to send the table, e.g.:\nopen(\"file.tex\", \"w\") do io\n  latex_table(io, df)\nend\nIf left out, io defaults to stdout.\ndf::DataFrame: Dataframe of a solver. Each row is a problem.\n\nKeyword arguments:\n\ncols::Array{Symbol}: Which columns of the df. Defaults to using all columns;\nignore_missing_cols::Bool: If true, filters out the columns in cols that don't exist in the data frame. Useful when creating tables for solvers in a loop where one solver has a column the other doesn't. If false, throws BoundsError in that situation.\nfmt_override::Dict{Symbol,Function}: Overrides format for a specific column, such as\nfmt_override=Dict(:name => x->@sprintf(\"\\textbf{%s}\", x) |> safe_latex_AbstractString)`\nhdr_override::Dict{Symbol,String}: Overrides header names, such as hdr_override=Dict(:name => \"Name\"), where LaTeX escaping should be used if necessary.\n\nWe recommend using the safe_latex_foo functions when overriding formats, unless you're sure you don't need them.\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.markdown_table-Tuple{IO, DataFrames.DataFrame}","page":"Reference","title":"SolverBenchmark.markdown_table","text":"markdown_table(io, df, kwargs...)\n\nCreate a markdown table from a DataFrame using PrettyTables and format the output.\n\nInputs:\n\nio::IO: where to send the table, e.g.:\nopen(\"file.md\", \"w\") do io\n  markdown_table(io, df)\nend\nIf left out, io defaults to stdout.\ndf::DataFrame: Dataframe of a solver. Each row is a problem.\n\nKeyword arguments:\n\nhl: a highlighter or tuple of highlighters to color individual cells (when output to screen).       By default, we use a simple passfail_highlighter.\nall other keyword arguments are passed directly to format_table.\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.safe_latex_AbstractFloat-Tuple{AbstractString}","page":"Reference","title":"SolverBenchmark.safe_latex_AbstractFloat","text":"safe_latex_AbstractFloat(s::AbstractString)\n\nFormat the string representation of floats for output in a LaTeX table. Replaces infinite values with the \\infty LaTeX sequence. If the float is represented in exponential notation, the mantissa and exponent are wrapped in math delimiters. Otherwise, the entire float is wrapped in math delimiters.\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.safe_latex_AbstractFloat_col-Tuple{Integer}","page":"Reference","title":"SolverBenchmark.safe_latex_AbstractFloat_col","text":"safe_latex_AbstractFloat_col(col::Integer)\n\nGenerate a PrettyTables LaTeX formatter for real numbers.\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.safe_latex_AbstractString-Tuple{AbstractString}","page":"Reference","title":"SolverBenchmark.safe_latex_AbstractString","text":"safe_latex_AbstractString(s::AbstractString)\n\nFormat a string for output in a LaTeX table. Escapes underscores.\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.safe_latex_AbstractString_col-Tuple{Integer}","page":"Reference","title":"SolverBenchmark.safe_latex_AbstractString_col","text":"safe_latex_AbstractString_col(col:::Integer)\n\nGenerate a PrettyTables LaTeX formatter for strings. Replaces _ with \\_.\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.safe_latex_Signed-Tuple{AbstractString}","page":"Reference","title":"SolverBenchmark.safe_latex_Signed","text":"safe_latex_Signed(s::AbstractString)\n\nFormat the string representation of signed integers for output in a LaTeX table. Encloses s in \\( and \\).\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.safe_latex_Signed_col-Tuple{Integer}","page":"Reference","title":"SolverBenchmark.safe_latex_Signed_col","text":"safe_latex_Signed_col(col::Integer)\n\nGenerate a PrettyTables LaTeX formatter for signed integers.\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.safe_latex_Symbol-Tuple{Any}","page":"Reference","title":"SolverBenchmark.safe_latex_Symbol","text":"safe_latex_Symbol(s)\n\nFormat a symbol for output in a LaTeX table. Calls safe_latex_AbstractString(string(s)).\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.safe_latex_Symbol_col-Tuple{Integer}","page":"Reference","title":"SolverBenchmark.safe_latex_Symbol_col","text":"safe_latex_Symbol_col(col::Integer)\n\nGenerate a PrettyTables LaTeX formatter for symbols.\n\n\n\n\n\n","category":"method"},{"location":"#Home-1","page":"Home","title":"SolverBenchmark.jl documentation","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"This package provides general tools for benchmarking solvers, focusing on a few guidelines:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"The output of a solver's run on a suite of problems is a DataFrame, where each row is a different problem.\nSince naming issues may arise (e.g., same problem with different number of variables), there must be an ID column;\nThe collection of two or more solver runs (DataFrames), is a Dict{Symbol,DataFrame}, where each key is a solver;","category":"page"},{"location":"#","page":"Home","title":"Home","text":"This package is developed focusing on Krylov.jl and JSOSolvers.jl, but they should be general enough to be used in other places.","category":"page"},{"location":"tutorial/#Tutorial-1","page":"Tutorial","title":"Tutorial","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"In this tutorial we illustrate the main uses of SolverBenchmark.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"First, let's create fake data. It is imperative that the data for each solver be stored in DataFrames, and the collection of different solver must be stored in a dictionary of Symbol to DataFrame.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"In our examples we'll use the following data.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"using DataFrames, Printf, Random\n\nRandom.seed!(0)\n\nn = 10\nnames = [:alpha, :beta, :gamma]\nstats = Dict(name => DataFrame(:id => 1:n,\n         :name => [@sprintf(\"prob%03d\", i) for i = 1:n],\n         :status => map(x -> x < 0.75 ? :first_order : :failure, rand(n)),\n         :f => randn(n),\n         :t => 1e-3 .+ rand(n) * 1000,\n         :iter => rand(10:10:100, n),\n         :irrelevant => randn(n)) for name in names)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"The data consists of a (fake) run of three solvers alpha, beta and gamma. Each solver has a column id, which is necessary for joining the solvers (names can be repeated), and columns name, status, f, t and iter corresponding to problem results. There is also a column irrelevant with extra information that will not be used to produce our benchmarks.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Here are the statistics of solver alpha:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"stats[:alpha]","category":"page"},{"location":"tutorial/#Tables-1","page":"Tutorial","title":"Tables","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"The first thing we may want to do is produce a table for each solver. Notice that the solver result is already a DataFrame, so there are a few options available in other packages, as well as simply printing the DataFrame. Our concern here is two-fold: producing publication-ready LaTeX tables, and web-ready markdown tables.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"The simplest use is pretty_stats(io, dataframe). By default, io is stdout:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"using SolverBenchmark\n\npretty_stats(stats[:alpha])","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Printing is LaTeX format is achieved with pretty_latex_stats:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"pretty_latex_stats(stats[:alpha])","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Alternatively, you can print to a file.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"open(\"alpha.tex\", \"w\") do io\n  println(io, \"\\\\documentclass[varwidth=20cm,crop=true]{standalone}\")\n  println(io, \"\\\\usepackage{longtable}\")\n  println(io, \"\\\\begin{document}\")\n  pretty_latex_stats(io, stats[:alpha])\n  println(io, \"\\\\end{document}\")\nend","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"run(`latexmk -quiet -pdf alpha.tex`)\nrun(`pdf2svg alpha.pdf alpha.svg`)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"(Image: )","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"If only a subset of columns should be printed, the DataFrame should be indexed accordingly:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"df = stats[:alpha]\npretty_stats(df[!, [:name, :f, :t]])","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Markdown tables may be generated by supplying the PrettyTables tf keyword argument to specify the table format:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"pretty_stats(df[!, [:name, :f, :t]], tf=tf_markdown)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"All values of tf accepted by PrettyTables may be used in SolverBenchmark.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"The fmt_override option overrides the formatting of a specific column. The argument should be a dictionary of Symbol to format strings, where the format string will be applied to each element of the column.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"The hdr_override changes the column headers.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"fmt_override = Dict(:f => \"%+10.3e\",\n                    :t => \"%08.2f\")\nhdr_override = Dict(:name => \"Name\", :f => \"f(x)\", :t => \"Time\")\npretty_stats(stdout,\n             df[!, [:name, :f, :t]],\n             col_formatters = fmt_override,\n             hdr_override = hdr_override)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"While col_formatters is for simple format strings, the PrettyTables API lets us define more elaborate formatters in the form of functions:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"fmt_override = Dict(:f => \"%+10.3e\",\n                    :t => \"%08.2f\")\nhdr_override = Dict(:name => \"Name\", :f => \"f(x)\", :t => \"Time\")\npretty_stats(df[!, [:name, :f, :t]],\n             col_formatters = fmt_override,\n             hdr_override = hdr_override,\n             formatters = (v, i, j) -> begin\n               if j == 3  # t is the 3rd column\n                 vi = floor(Int, v)\n                 minutes = div(vi, 60)\n                 seconds = vi % 60\n                 micros = round(Int, 1e6 * (v - vi))\n                 @sprintf(\"%2dm %02ds %06dμs\", minutes, seconds, micros)\n               else\n                 v\n               end\n             end)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"See the PrettyTables.jl documentation for more information.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"When using LaTeX format, the output must be understood by LaTeX. By default, numerical data in the table is wrapped in inline math environments. But those math environments would interfere with our formatting of the time. Thus we must first disable them for the time column using col_formatters, and then apply the PrettyTables formatter as above:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"fmt_override = Dict(:f => \"%+10.3e\",\n                    :t => \"%08.2f\")\nhdr_override = Dict(:name => \"Name\", :f => \"f(x)\", :t => \"Time\")\nopen(\"alpha2.tex\", \"w\") do io\n  println(io, \"\\\\documentclass[varwidth=20cm,crop=true]{standalone}\")\n  println(io, \"\\\\usepackage{longtable}\")\n  println(io, \"\\\\begin{document}\")\n  pretty_latex_stats(io,\n                    df[!, [:name, :status, :f, :t, :iter]],\n                    col_formatters = Dict(:t => \"%f\"),  # disable default formatting of t\n                    formatters = (v,i,j) -> begin\n                      if j == 4\n                        xi = floor(Int, v)\n                        minutes = div(xi, 60)\n                        seconds = xi % 60\n                        micros = round(Int, 1e6 * (v - xi))\n                        @sprintf(\"\\\\(%2d\\\\)m \\\\(%02d\\\\)s \\\\(%06d \\\\mu\\\\)s\", minutes, seconds, micros)\n                      else\n                        v\n                      end\n                  end)\n  println(io, \"\\\\end{document}\")\nend","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"run(`latexmk -quiet -pdf alpha2.tex`)\nrun(`pdf2svg alpha2.pdf alpha2.svg`)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"(Image: )","category":"page"},{"location":"tutorial/#Joining-tables-1","page":"Tutorial","title":"Joining tables","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"In some occasions, instead of/in addition to showing individual results, we show a table with the result of multiple solvers.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"df = join(stats, [:f, :t])\npretty_stats(stdout, df)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"The column :id is used as guide on where to join. In addition, we may have repeated columns between the solvers. We convery that information with argument invariant_cols.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"df = join(stats, [:f, :t], invariant_cols=[:name])\npretty_stats(stdout, df)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"join also accepts hdr_override for changing the column name before appending _solver.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"hdr_override = Dict(:name => \"Name\", :f => \"f(x)\", :t => \"Time\")\ndf = join(stats, [:f, :t], invariant_cols=[:name], hdr_override=hdr_override)\npretty_stats(stdout, df)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"hdr_override = Dict(:name => \"Name\", :f => \"\\\\(f(x)\\\\)\", :t => \"Time\")\ndf = join(stats, [:f, :t], invariant_cols=[:name], hdr_override=hdr_override)\nopen(\"alpha3.tex\", \"w\") do io\n  println(io, \"\\\\documentclass[varwidth=20cm,crop=true]{standalone}\")\n  println(io, \"\\\\usepackage{longtable}\")\n  println(io, \"\\\\begin{document}\")\n  pretty_latex_stats(io, df)\n  println(io, \"\\\\end{document}\")\nend","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"run(`latexmk -quiet -pdf alpha3.tex`)\nrun(`pdf2svg alpha3.pdf alpha3.svg`)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"(Image: )","category":"page"},{"location":"tutorial/#Profiles-1","page":"Tutorial","title":"Profiles","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Performance profiles are a comparison tool developed by Dolan and Moré, 2002 that takes into account the relative performance of a solver and whether it has achieved convergence for each problem. SolverBenchmark.jl uses BenchmarkProfiles.jl for generating performance profiles from the dictionary of DataFrames.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"The basic usage is performance_profile(stats, cost), where cost is a function applied to a DataFrame and returning a vector.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"# Running on setup to avoid warnings\nusing Plots\npyplot()\n\np = performance_profile(stats, df -> df.t)\nPlots.svg(p, \"profile1\")","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"using Plots\npyplot()\n\np = performance_profile(stats, df -> df.t)\nPlots.svg(p, \"profile1\")","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"(Image: )","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Notice that we used df -> df.t which corresponds to the column :t of the DataFrames. This does not take into account that the solvers have failed for a few problems (according to column :status). The next profile takes that into account.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"cost(df) = (df.status .!= :first_order) * Inf + df.t\np = performance_profile(stats, cost)\nPlots.svg(p, \"profile2\")","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"cost(df) = (df.status .!= :first_order) * Inf + df.t\np = performance_profile(stats, cost)\nPlots.svg(p, \"profile2\")","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"(Image: )","category":"page"},{"location":"tutorial/#Profile-wall-1","page":"Tutorial","title":"Profile wall","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Another profile function is profile_solvers, which creates a wall of performance profiles, accepting multiple costs and doing 1 vs 1 comparisons in addition to the traditional performance profile.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"solved(df) = (df.status .== :first_order)\ncosts = [df -> .!solved(df) * Inf + df.t, df -> .!solved(df) * Inf + df.iter]\ncostnames = [\"Time\", \"Iterations\"]\np = profile_solvers(stats, costs, costnames)\nPlots.svg(p, \"profile3\")","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"solved(df) = (df.status .== :first_order)\ncosts = [df -> .!solved(df) * Inf + df.t, df -> .!solved(df) * Inf + df.iter]\ncostnames = [\"Time\", \"Iterations\"]\np = profile_solvers(stats, costs, costnames)\nPlots.svg(p, \"profile3\")","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"(Image: )","category":"page"}]
}
