var documenterSearchIndex = {"docs":
[{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/#Contents","page":"Reference","title":"Contents","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Pages = [\"reference.md\"]","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/#Index","page":"Reference","title":"Index","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Pages = [\"reference.md\"]","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [SolverBenchmark]","category":"page"},{"location":"reference/#Base.join-Tuple{Dict{Symbol, DataFrames.DataFrame}, Vector{Symbol}}","page":"Reference","title":"Base.join","text":"df = join(stats, cols; kwargs...)\n\nJoin a dictionary of DataFrames given by stats. Column :id is required in all DataFrames. The resulting DataFrame will have column id and all columns cols for each solver.\n\nInputs:\n\nstats::Dict{Symbol,DataFrame}: Dictionary of DataFrames per solver. Each key is a different solver;\ncols::Array{Symbol}: Which columns of the DataFrames.\n\nKeyword arguments:\n\ninvariant_cols::Array{Symbol,1}: Invariant columns to be added, i.e., columns that don't change depending on the solver (such as name of problem, number of variables, etc.);\nhdr_override::Dict{Symbol,String}: Override header names.\n\nOutput:\n\ndf::DataFrame: Resulting dataframe.\n\n\n\n\n\n","category":"method"},{"location":"reference/#BenchmarkProfiles.performance_profile-Tuple{AbstractDict{Symbol, DataFrames.DataFrame}, Function, Vararg{Any}}","page":"Reference","title":"BenchmarkProfiles.performance_profile","text":"performance_profile(stats, cost, args...; b = PlotsBackend(), kwargs...)\n\nProduce a performance profile comparing solvers in stats using the cost function.\n\nInputs:\n\nstats::AbstractDict{Symbol,DataFrame}: pairs of :solver => df;\ncost::Function: cost function applyed to each df. Should return a vector with the cost of solving the problem at each row;\n0 cost is not allowed;\nIf the solver did not solve the problem, return Inf or a negative number.\nb::BenchmarkProfiles.AbstractBackend : backend used for the plot.\n\nIf several profiles will be produced with variants of the same solvers, stats may be an OrderedDict, as defined in the OrderedCollections.jl package.\n\nExamples of cost functions:\n\ncost(df) = df.elapsed_time: Simple elapsed_time cost. Assumes the solver solved the problem.\ncost(df) = (df.status .!= :first_order) * Inf + df.elapsed_time: Takes into consideration the status of the solver.\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.LTXformat","page":"Reference","title":"SolverBenchmark.LTXformat","text":"LTXformat(x)\n\nFormat x according to its type. For types Signed, AbstractFloat, AbstractString and Symbol, use a predefined formatting string passed to @sprintf and then the corresponding safe_latex_<type> function.\n\nFor type Missing, return \"NA\".\n\n\n\n\n\n","category":"function"},{"location":"reference/#SolverBenchmark.MDformat","page":"Reference","title":"SolverBenchmark.MDformat","text":"MDformat(x)\n\nFormat x according to its type. For types Signed, AbstractFloat, AbstractString and Symbol, use a predefined formatting string passed to @sprintf.\n\nFor type Missing, return \"NA\".\n\n\n\n\n\n","category":"function"},{"location":"reference/#SolverBenchmark.bmark_results_to_dataframes-Tuple{PkgBenchmark.BenchmarkResults}","page":"Reference","title":"SolverBenchmark.bmark_results_to_dataframes","text":"stats = bmark_results_to_dataframes(results)\n\nConvert PkgBenchmark results to a dictionary of DataFrames. The benchmark SUITE should have been constructed in the form\n\nSUITE[solver][case] = ...\n\nwhere solver will be recorded as one of the solvers to be compared in the DataFrame and case is a test case. For example:\n\nSUITE[\"CG\"][\"BCSSTK09\"] = @benchmarkable ...\nSUITE[\"LBFGS\"][\"ROSENBR\"] = @benchmarkable ...\n\nInputs:\n\nresults::BenchmarkResults: the result of PkgBenchmark.benchmarkpkg\n\nOutput:\n\nstats::Dict{Symbol,DataFrame}: a dictionary of DataFrames containing the   benchmark results per solver.\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.bmark_solvers-Tuple{Dict{Symbol}, Vararg{Any}}","page":"Reference","title":"SolverBenchmark.bmark_solvers","text":"bmark_solvers(solvers :: Dict{Symbol,Any}, args...; kwargs...)\n\nRun a set of solvers on a set of problems.\n\nArguments\n\nsolvers: a dictionary of solvers to which each problem should be passed\nother positional arguments accepted by solve_problems, except for a solver name\n\nKeyword arguments\n\nAny keyword argument accepted by solve_problems\n\nReturn value\n\nA Dict{Symbol, AbstractExecutionStats} of statistics.\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.count_unique-Tuple{Any}","page":"Reference","title":"SolverBenchmark.count_unique","text":"vals = count_unique(X)\n\nCount the number of occurrences of each value in X.\n\nArguments\n\nX: an iterable.\n\nReturn value\n\nA Dict{eltype(X),Int} whose keys are the unique elements in X and values are their number of occurrences.\n\nExample: the snippet\n\nstats = load_stats(\"mystats.jld2\")\nfor solver ∈ keys(stats)\n  @info \"$solver statuses\" count_unique(stats[solver].status)\nend\n\ndisplays the number of occurrences of each final status for each solver in stats.\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.format_table-Union{Tuple{F}, Tuple{DataFrames.DataFrame, Function}} where F<:Function","page":"Reference","title":"SolverBenchmark.format_table","text":"format_table(df, formatter, kwargs...)\n\nFormat the data frame into a table using formatter. Used by other table functions.\n\nInputs:\n\ndf::DataFrame: Dataframe of a solver. Each row is a problem.\nformatter::Function: A function that formats its input according to its type. See LTXformat or MDformat for examples.\n\nKeyword arguments:\n\ncols::Array{Symbol}: Which columns of the df. Defaults to using all columns;\nignore_missing_cols::Bool: If true, filters out the columns in cols that don't exist in the data frame. Useful when creating tables for solvers in a loop where one solver has a column the other doesn't. If false, throws BoundsError in that situation.\nfmt_override::Dict{Symbol,Function}: Overrides format for a specific column, such as\nfmt_override=Dict(:name => x->@sprintf(\"%-10s\", x))\nhdr_override::Dict{Symbol,String}: Overrides header names, such as hdr_override=Dict(:name => \"Name\").\n\nOutputs:\n\nheader::Array{String,1}: header vector.\ntable::Array{String,2}: formatted table.\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.gradient_highlighter-Tuple{DataFrames.DataFrame, Symbol}","page":"Reference","title":"SolverBenchmark.gradient_highlighter","text":"hl = gradient_highlighter(df, col; cmap=:coolwarm)\n\nA PrettyTables highlighter the applies a color gradient to the values in columns given by cols.\n\nInput Arguments\n\ndf::DataFrame dataframe to which the highlighter will be applied;\ncol::Symbol a symbol to indicate which column the highlighter will be applied to.\n\nKeyword Arguments\n\ncmap::Symbol color scheme to use, from ColorSchemes.\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.judgement_results_to_dataframes-Tuple{PkgBenchmark.BenchmarkJudgement}","page":"Reference","title":"SolverBenchmark.judgement_results_to_dataframes","text":"stats = judgement_results_to_dataframes(judgement)\n\nConvert BenchmarkJudgement results to a dictionary of DataFrames.\n\nInputs:\n\njudgement::BenchmarkJudgement: the result of, e.g.,\ncommit = benchmarkpkg(mypkg)  # benchmark a commit or pull request\nmain = benchmarkpkg(mypkg, \"main\")  # baseline benchmark\njudgement = judge(commit, main)\n\nOutput:\n\nstats::Dict{Symbol,Dict{Symbol,DataFrame}}: a dictionary of   Dict{Symbol,DataFrame}s containing the target and baseline benchmark results.   The elements of this dictionary are the same as those returned by   bmark_results_to_dataframes(main) and bmark_results_to_dataframes(commit).\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.latex_table-Tuple{IO, DataFrames.DataFrame}","page":"Reference","title":"SolverBenchmark.latex_table","text":"latex_table(io, df, kwargs...)\n\nCreate a latex longtable of a DataFrame using LaTeXTabulars, and format the output for a publication-ready table.\n\nInputs:\n\nio::IO: where to send the table, e.g.:\nopen(\"file.tex\", \"w\") do io\n  latex_table(io, df)\nend\nIf left out, io defaults to stdout.\ndf::DataFrame: Dataframe of a solver. Each row is a problem.\n\nKeyword arguments:\n\ncols::Array{Symbol}: Which columns of the df. Defaults to using all columns;\nignore_missing_cols::Bool: If true, filters out the columns in cols that don't exist in the data frame. Useful when creating tables for solvers in a loop where one solver has a column the other doesn't. If false, throws BoundsError in that situation.\nfmt_override::Dict{Symbol,Function}: Overrides format for a specific column, such as\nfmt_override=Dict(:name => x->@sprintf(\"\\textbf{%s}\", x) |> safe_latex_AbstractString)`\nhdr_override::Dict{Symbol,String}: Overrides header names, such as hdr_override=Dict(:name => \"Name\"), where LaTeX escaping should be used if necessary.\n\nWe recommend using the safe_latex_foo functions when overriding formats, unless you're sure you don't need them.\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.load_stats-Tuple{AbstractString}","page":"Reference","title":"SolverBenchmark.load_stats","text":"stats = load_stats(filename; kwargs...)\n\nArguments\n\nfilename::AbstractString: the input file name.\n\nKeyword arguments\n\nkey::String=\"stats\": the key under which the data can be read in filename. The key should be the same as the one used when save_stats was called.\n\nReturn value\n\nA Dict{Symbol,DataFrame} containing the statistics stored in file filename. The user should import DataFrames before calling load_stats.\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.markdown_table-Tuple{IO, DataFrames.DataFrame}","page":"Reference","title":"SolverBenchmark.markdown_table","text":"markdown_table(io, df, kwargs...)\n\nCreate a markdown table from a DataFrame using PrettyTables and format the output.\n\nInputs:\n\nio::IO: where to send the table, e.g.:\nopen(\"file.md\", \"w\") do io\n  markdown_table(io, df)\nend\nIf left out, io defaults to stdout.\ndf::DataFrame: Dataframe of a solver. Each row is a problem.\n\nKeyword arguments:\n\nhl: a highlighter or tuple of highlighters to color individual cells (when output to screen).       By default, we use a simple passfail_highlighter.\nall other keyword arguments are passed directly to format_table.\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.passfail_highlighter","page":"Reference","title":"SolverBenchmark.passfail_highlighter","text":"hl = passfail_highlighter(df, c=crayon\"bold red\")\n\nA PrettyTables highlighter that colors failures in bold red by default.\n\nInput Arguments\n\ndf::DataFrame dataframe to which the highlighter will be applied.   df must have the id column.\n\nIf df has the :status property, the highlighter will be applied to rows for which df.status indicates a failure. A failure is any status different from :first_order or :unbounded.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SolverBenchmark.passfail_latex_highlighter","page":"Reference","title":"SolverBenchmark.passfail_latex_highlighter","text":"hl = passfail_latex_highlighter(df)\n\nA PrettyTables LaTeX highlighter that colors failures in bold red by default.\n\nSee the documentation of passfail_highlighter for more information.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SolverBenchmark.pretty_latex_stats-Tuple{IO, DataFrames.DataFrame}","page":"Reference","title":"SolverBenchmark.pretty_latex_stats","text":"pretty_latex_stats(df; kwargs...)\n\nPretty-print a DataFrame as a LaTeX longtable using PrettyTables.\n\nSee the pretty_stats documentation. Specific settings in this method are:\n\nthe backend is set to :latex;\nthe table type is set to :longtable;\nhighlighters, if any, should be LaTeX highlighters.\n\nSee the PrettyTables documentation for more information.\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.pretty_stats-Tuple{IO, DataFrames.DataFrame}","page":"Reference","title":"SolverBenchmark.pretty_stats","text":"pretty_stats(df; kwargs...)\n\nPretty-print a DataFrame using PrettyTables.\n\nArguments\n\nio::IO: an IO stream to which the table will be output (default: stdout);\ndf::DataFrame: the DataFrame to be displayed. If only certain columns of df should be displayed,     they should be extracted explicitly, e.g., by passing df[!, [:col1, :col2, :col3]].\n\nKeyword Arguments\n\ncol_formatters::Dict{Symbol, String}: a Dict of format strings to apply to selected columns of df.     The keys of col_formatters should be symbols, so that specific formatting can be applied to specific columns.     By default, default_formatters is used, based on the column type.     If PrettyTables formatters are passed using the formatters keyword argument, they are applied     before those in col_formatters.\nhdr_override::Dict{Symbol, String}: a Dict of those headers that should be displayed differently than     simply according to the column name (default: empty). Example: Dict(:col1 => \"column 1\").\n\nAll other keyword arguments are passed directly to pretty_table. In particular,\n\nuse tf=tf_markdown to display a Markdown table;\ndo not use this function for LaTeX output; use pretty_latex_stats instead;\nany PrettyTables highlighters can be given, but see the predefined passfail_highlighter and gradient_highlighter.\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.profile_package-Tuple{PkgBenchmark.BenchmarkJudgement}","page":"Reference","title":"SolverBenchmark.profile_package","text":"p = profile_package(judgement)\n\nProduce performance profiles based on PkgBenchmark.BenchmarkJudgement results.\n\nInputs:\n\njudgement::BenchmarkJudgement: the result of, e.g.,\ncommit = benchmarkpkg(mypkg)  # benchmark a commit or pull request\nmain = benchmarkpkg(mypkg, \"main\")  # baseline benchmark\njudgement = judge(commit, main)\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.profile_solvers-Tuple{AbstractDict{Symbol, DataFrames.DataFrame}, Vector{<:Function}, Vector{String}}","page":"Reference","title":"SolverBenchmark.profile_solvers","text":"p = profile_solvers(stats, costs, costnames;\n                    width = 400, height = 400,\n                    b = PlotsBackend(), kwargs...)\n\nProduce performance profiles comparing solvers based on the data in stats.\n\nInputs:\n\nstats::AbstractDict{Symbol,DataFrame}: a dictionary of DataFrames containing the   benchmark results per solver (e.g., produced by bmark_results_to_dataframes())\ncosts::Vector{Function}: a vector of functions specifying the measures to use in the profiles\ncostnames::Vector{String}: names to be used as titles of the profiles.\n\nKeyword inputs:\n\nwidth::Int: Width of each individual plot (Default: 400)\nheight::Int: Height of each individual plot (Default: 400)\nb::BenchmarkProfiles.AbstractBackend : backend used for the plot.\n\nAdditional kwargs are passed to the plot call.\n\nOutput: A Plots.jl plot representing a set of performance profiles comparing the solvers. The set contains performance profiles comparing all the solvers together on the measures given in costs. If there are more than two solvers, additional profiles are produced comparing the solvers two by two on each cost measure.\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.profile_solvers-Tuple{PkgBenchmark.BenchmarkResults}","page":"Reference","title":"SolverBenchmark.profile_solvers","text":"p = profile_solvers(results)\n\nProduce performance profiles based on PkgBenchmark.benchmarkpkg results.\n\nInputs:\n\nresults::BenchmarkResults: the result of PkgBenchmark.benchmarkpkg.\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.quick_summary-Tuple{Dict{Symbol, DataFrames.DataFrame}}","page":"Reference","title":"SolverBenchmark.quick_summary","text":"statuses, avgs = quick_summary(stats; kwargs...)\n\nCall count_unique and compute a few average measures for each solver in stats.\n\nArguments\n\nstats::Dict{Symbol,DataFrame}: benchmark statistics such as returned by bmark_solvers.\n\nKeyword arguments\n\ncols::Vector{Symbol}: symbols indicating DataFrame columns in solver statistics for which we compute averages. Default: [:iter, :neval_obj, :neval_grad, :neval_hess, :neval_hprod, :elapsed_time].\n\nReturn value\n\nstatuses::Dict{Symbol,Dict{Symbol,Int}}: a dictionary of number of occurrences of each final status for each solver in stats. Each value in this dictionary is returned by count_unique\navgs::Dict{Symbol,Dict{Symbol,Float64}}: a dictionary that contains averages of performance measures across all problems for each solver. Each avgs[solver] is a Dict{Symbol,Float64} where the measures are those given in the keyword argument cols and values are averages of those measures across all problems.\n\nExample: the snippet\n\nstatuses, avgs = quick_summary(stats)\nfor solver ∈ keys(stats)\n  @info \"statistics for\" solver statuses[solver] avgs[solver]\nend\n\ndisplays quick summary and averages for each solver.\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.safe_latex_AbstractFloat-Tuple{AbstractString}","page":"Reference","title":"SolverBenchmark.safe_latex_AbstractFloat","text":"safe_latex_AbstractFloat(s::AbstractString)\n\nFormat the string representation of floats for output in a LaTeX table. Replaces infinite values with the \\infty LaTeX sequence. If the float is represented in exponential notation, the mantissa and exponent are wrapped in math delimiters. Otherwise, the entire float is wrapped in math delimiters.\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.safe_latex_AbstractFloat_col-Tuple{Integer}","page":"Reference","title":"SolverBenchmark.safe_latex_AbstractFloat_col","text":"safe_latex_AbstractFloat_col(col::Integer)\n\nGenerate a PrettyTables LaTeX formatter for real numbers.\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.safe_latex_AbstractString-Tuple{AbstractString}","page":"Reference","title":"SolverBenchmark.safe_latex_AbstractString","text":"safe_latex_AbstractString(s::AbstractString)\n\nFormat a string for output in a LaTeX table. Escapes underscores.\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.safe_latex_AbstractString_col-Tuple{Integer}","page":"Reference","title":"SolverBenchmark.safe_latex_AbstractString_col","text":"safe_latex_AbstractString_col(col:::Integer)\n\nGenerate a PrettyTables LaTeX formatter for strings. Replaces _ with \\_.\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.safe_latex_Signed-Tuple{AbstractString}","page":"Reference","title":"SolverBenchmark.safe_latex_Signed","text":"safe_latex_Signed(s::AbstractString)\n\nFormat the string representation of signed integers for output in a LaTeX table. Encloses s in \\( and \\).\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.safe_latex_Signed_col-Tuple{Integer}","page":"Reference","title":"SolverBenchmark.safe_latex_Signed_col","text":"safe_latex_Signed_col(col::Integer)\n\nGenerate a PrettyTables LaTeX formatter for signed integers.\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.safe_latex_Symbol-Tuple{Any}","page":"Reference","title":"SolverBenchmark.safe_latex_Symbol","text":"safe_latex_Symbol(s)\n\nFormat a symbol for output in a LaTeX table. Calls safe_latex_AbstractString(string(s)).\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.safe_latex_Symbol_col-Tuple{Integer}","page":"Reference","title":"SolverBenchmark.safe_latex_Symbol_col","text":"safe_latex_Symbol_col(col::Integer)\n\nGenerate a PrettyTables LaTeX formatter for symbols.\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.save_stats-Tuple{Dict{Symbol, DataFrames.DataFrame}, AbstractString}","page":"Reference","title":"SolverBenchmark.save_stats","text":"save_stats(stats, filename; kwargs...)\n\nWrite the benchmark statistics stats to a file named filename.\n\nArguments\n\nstats::Dict{Symbol,DataFrame}: benchmark statistics such as returned by bmark_solvers\nfilename::AbstractString: the output file name.\n\nKeyword arguments\n\nforce::Bool=false: whether to overwrite filename if it already exists\nkey::String=\"stats\": the key under which the data can be read from filename later.\n\nReturn value\n\nThis method returns an error if filename exists and force==false. On success, it returns the value of jldopen(filename, \"w\").\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.solve_problems-Union{Tuple{TName}, Tuple{Any, TName, Any}} where TName","page":"Reference","title":"SolverBenchmark.solve_problems","text":"solve_problems(solver, solver_name, problems; kwargs...)\n\nApply a solver to a set of problems.\n\nArguments\n\nsolver: the function name of a solver;\nsolver_name: name of the solver;\nproblems: the set of problems to pass to the solver, as an iterable of AbstractNLPModel. It is recommended to use a generator expression (necessary for CUTEst problems).\n\nKeyword arguments\n\nsolver_logger::AbstractLogger: logger wrapping the solver call (default: NullLogger);\nreset_problem::Bool: reset the problem's counters before solving (default: true);\nskipif::Function: function to be applied to a problem and return whether to skip it (default: x->false);\ncolstats::Vector{Symbol}: summary statistics for the logger to output during the\n\nbenchmark (default: [:name, :nvar, :ncon, :status, :elapsed_time, :objective, :dual_feas, :primal_feas]);\n\ninfo_hdr_override::Dict{Symbol,String}: header overrides for the summary statistics (default: use default headers);\nprune: do not include skipped problems in the final statistics (default: true);\nany other keyword argument to be passed to the solver.\n\nReturn value\n\na DataFrame where each row is a problem, minus the skipped ones if prune is true.\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.to_gist-Tuple{Any}","page":"Reference","title":"SolverBenchmark.to_gist","text":"posted_gist = to_gist(results)\n\nCreate and post a gist with the benchmark results and performance profiles.\n\nInputs:\n\nresults::BenchmarkResults: the result of PkgBenchmark.benchmarkpkg\n\nOutput:\n\nthe return value of GitHub.jl's create_gist.\n\n\n\n\n\n","category":"method"},{"location":"reference/#SolverBenchmark.to_gist-Tuple{PkgBenchmark.BenchmarkResults, Any}","page":"Reference","title":"SolverBenchmark.to_gist","text":"posted_gist = to_gist(results, p)\n\nCreate and post a gist with the benchmark results and performance profiles.\n\nInputs:\n\nresults::BenchmarkResults: the result of PkgBenchmark.benchmarkpkg\np:: the result of profile_solvers.\n\nOutput:\n\nthe return value of GitHub.jl's create_gist.\n\n\n\n\n\n","category":"method"},{"location":"#Home","page":"Home","title":"SolverBenchmark.jl documentation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package provides general tools for benchmarking solvers, focusing on a few guidelines:","category":"page"},{"location":"","page":"Home","title":"Home","text":"The output of a solver's run on a suite of problems is a DataFrame, where each row is a different problem.\nSince naming issues may arise (e.g., same problem with different number of variables), there must be an ID column;\nThe collection of two or more solver runs (DataFrames), is a Dict{Symbol,DataFrame}, where each key is a solver;","category":"page"},{"location":"","page":"Home","title":"Home","text":"This package is developed focusing on Krylov.jl and JSOSolvers.jl, but they should be general enough to be used in other places.","category":"page"},{"location":"#Bug-reports-and-discussions","page":"Home","title":"Bug reports and discussions","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you think you found a bug, feel free to open an issue. Focused suggestions and requests can also be opened as issues. Before opening a pull request, start an issue or a discussion on the topic, please.","category":"page"},{"location":"","page":"Home","title":"Home","text":"If you want to ask a question not suited for a bug report, feel free to start a discussion here. This forum is for general discussion about this repository and the JuliaSmoothOptimizers organization, so questions about any of our packages are welcome.","category":"page"},{"location":"#Tutorials","page":"Home","title":"Tutorials","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"You can access more tutorials regarding SolverBenchmark.jl on the JuliaSmoothOptimizers website jso.dev, for instance, an Introduction to SolverBenchmark.","category":"page"}]
}
